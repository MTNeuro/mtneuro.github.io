<!DOCTYPE html>
<html>

    <!-- TO DO:
    
    Remove old data access Github links
    Change data access Jupyter notebook links

    Work on JavaScript!

    -->

    <head>
        <title>MTNeuro</title>
        <link rel="stylesheet" type="text/css" href="styles.css" />
        <link rel="icon" type="image/x-icon" href="images/favicon.ico">
    </head>
    <body>
        <div id="headers">
            <h1><strong>MTNeuro: A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of Abstraction</strong></h1>
            <br>
            <h2>[<a href="https://openreview.net/pdf?id=5xuowSQ17vy" target="_blank">Paper</a>] [<a href="https://github.com/MTNeuro/MTNeuro" target="_blank">Github</a>] [<a href="https://bossdb.org/project/prasad2020" target="_blank">Dataset</a>]</h2>
        </div>
        <p><img src="images/dataset.png" alt="Dataset" id="dataset-img"></p>
        <p><img src="images/dataset2.png" alt="Brain Regions" id="dataset2-img"></p>
        <hr>
        <h2><strong>Abstract</strong></h2>
        <p>MTNeuro introduces a new dataset, annotations, and multiple downstream tasks that provide diverse ways to readout <em>both</em> 
            macroscopic and microscopic brain architectural information from the same image. This multi-task neuroimaging benchmark is built on 
            volumetric, micrometer-resolution X-ray microtomography imaging of a large thalamocortical section of mouse brain, which encompasses 
            multiple cortical and subcortical regions and reveals dense reconstructions of the underlying microstructure (i.e., cell bodies, 
            vasculature, and axons). We generated a number of different prediction challenges and evaluated several supervised and self-supervised 
            models for brain-region prediction and pixel-level semantic microstructural segmentation. Our experiments not only highlight the rich 
            heterogeneity of this dataset, but also provide insights into how self-supervised approaches can be used to learn representations 
            that capture multiple attributes of a single image and perform well on a variety of downstream tasks.</p>
        <hr>
        <h2><strong>Key Features</strong></h2>
        <ul>
            <li><strong>Three Dimensional Multi-Scale Annotated Dataset:</strong>  The 3D x-ray microtomography dataset spans multiple brain areas and includes region of interest (ROI) annotations, densely annotated 3D cutouts, and semantic interpretable features.</li>
            <li><strong>Multi-Level Benchmark Tasks:</strong> Benchmark tasks feature both microscopic and macroscopic classification objectives.</li>
            <li><strong>Evaluation of Model Baselines:</strong> Both 2D and 3D training regimes are considered when training supervised and unsupervised models.</li>
        </ul>
        <br>
        <hr>
        <h2><strong>Dataset</strong></h2>
        <p>We built our benchmark tasks on a large open access high-resolution (1.17μm isotropic) 3D microscopy database, which contains <strong>macroscopic-level ROI annotations</strong>, as well as 4 three-dimensional 256 × 256 × 360 cutouts from the somatosensory cortex (CTX), striatum (STR), ventral posterior region of thalamus (VP), and the zona incerta (ZI). These volumetric cutouts contain <strong>pixel-level microstuctural labels</strong>, identifying each point as either part of an axon, cell, blood vessel, or background.</p>
        <p>The dataset and all corresponding labels are stored publicly in <a href="https://bossdb.org/project/prasad2020" target="_blank">BossDB</a> and accessed on-demand with <a href="https://pypi.org/project/intern/" target="_blank">Intern</a>, a Python API library.</p>
        <p>Provided are a <a href="https://github.com/MTNeuro/MTNeuro/blob/main/MTNeuro/bossdbdataset.py" target="_blank">PyTorch DataLoader</a> for convenient algorithm
        development and testing, and example <a href="https://github.com/MTNeuro/MTNeuro/tree/main/notebooks" target="_blank">Jupyter Notebooks</a> to assist with downloading the task cutouts in other frameworks.</p>
        <p><img src="images/intern_and_dataloader.png" alt="Intern and DataLoader" id="intern-dataloader-img"></p>
        <hr>
        <h2><strong>Benchmark Tasks</strong></h2>
        <p><img src="images/tasks.png" alt="Tasks" id="tasks-img"></p>
        <br>
        <table style="width:100%">
            <thead>
                <tr>
                    <th>Task</th>
                    <th>Description</th>
                </tr>
            </thead>
            <tbody>
                <tr class="clickable" onclick="task1()">
                    <td>1. Image-Level Classification of Brain Area</td>
                    <td>Prediction of the brain region (somatosensory cortex, striatum, thalamus, zona incerta) to which a given image or volume belongs.</td>
                </tr>
                <tr class="clickable" onclick="task2()">
                    <td>2. Pixel-level Segmentation of Microstructures</td>
                    <td>Prediction of neural microstructures (blood vessels, axons, cell bodies, background) from pixel-level annotations within the four core brain regions contained in the dataset.</td>
                </tr>
                <tr class="clickable" onclick="task3()">
                    <td>3. Multi-task Decoding from Frozen Representations</td>
                    <td>Estimation of human-interpretable semantic features (such as the average cell size or axon density) from the representation of a given image or volume.</td>
                </tr>
            </tbody>
        </table>

        <p id="task-caption">An overview of the three benchmark tasks. <strong>Click</strong> each row for more information.</p>

        <div id="task-1">
            <h3><strong>Task 1</strong>: Image-Level Classification of Brain Area</h3>
            <strong>Goal: </strong>Classification of 4 brain regions:
            <ul>
                <li> ROI-C1: prediction of the origin brain region of a sample within a same subvolume
                <li> ROI-C2: evaluation of generalization of the prediction with different test subvolumes
                <li> ROI-C3: evaluation of performance of prediction when trained with more subvolumes
            </ul>
            <strong>Classes: </strong>Somatosensory Cortex (CTX), Striatum (STR), Thalamus (VP) and Zona Incerta (ZI)
            <br>
            <strong>Training sets:</strong>
            <ul>
                <li> ROI-C1: 256x256x300 sub-volumes A (blue) of the 4 brain regions
                <li> ROI-C2: 256x256x300 sub-volumes A (blue) of the 4 brain regions
                <li> ROI-C3: 256x256x300 sub-volumes A (blue) and 256x256x360 sub-volumes B (yellow) of the 4 brain regions
            </ul>
            <strong>Testing sets:</strong>
            <ul>
                <li> ROI-C1: 256x256x50 sub-volumes A of the 4 brain regions
                <li> ROI-C2: 256x256x50 sub-volumes B, C and D of the 4 brain regions
                <li> ROI-C3: 256x256x50 sub-volumes C and D of the 4 brain regions
            </ul>
            <strong>Modalities:</strong>
            <ul>
                <li> 3D classification: train with volumetric samples
                <li> 2D classification: train with image samples (the third dimension corresponds to the sample index)
            </ul>
            <p>Get started with executing Task 1 here: [<a href="https://github.com/MTNeuro/MTNeuro/blob/main/notebooks/task1_getting_started.ipynb" target="_blank">Jupyter Notebook</a>]</p>
            <br>
        </div>

        <div id="task-2">
            <h3><strong>Task 2</strong>: Pixel-level Segmentation of Microstructures</h3>
            <strong>Goal: </strong>Segmentation of 3 or 4 anatomical microstructures at the level of pixels
            <br>
            <strong>Classes: </strong>
            <ul>
                <li> 3 class segmentation: cell bodies, blood vessels, and axons + background
                <li> 4 class segmentation: cell bodies, blood vessels, axons, and background
            </ul>
            <strong>Training set:</strong> 256x256x300 sub-volumes of the 4 brain regions (3 regions (CTX, STR, VP) in the case of the 4 class segmentation)
            <br>
            <strong>Testing set:</strong> 256x256x50 sub-volumes of the 4 brain regions (3 regions (CTX, STR, VP) in the case of the 4 class segmentation)
            <br>
            <strong>Modality:</strong> 2D classification: train with image samples (the third dimension corresponds to the sample index)
            <br>
            <p>Get started with executing Task 2 here: [<a href="https://github.com/MTNeuro/MTNeuro/blob/main/notebooks/task2_getting_started.ipynb" target="_blank">Jupyter Notebook</a>]</p>
            <br>
        </div>

        <div id="task-3">
            <h3><strong>Task 3</strong>: Multi-task Decoding from Frozen Representations</h3>
            <strong>Goal: </strong>Learn representations to observe how well they encode semantic properties of volumes or how relevant these features are for a specific downstream task
            <br>
            <strong>Semantic properties examples: </strong>blood vessels pixels, cell counts, axon density and average distance between cells
            <br>
            <strong>Modality: </strong>Exploration of readout of contextual features from the latent space learned by self-supervised models in Task 1
            <br>
            <p>Get started with executing Task 3 here: [<a href="https://github.com/MTNeuro/MTNeuro/blob/main/notebooks/task3_getting_started.ipynb" target="_blank">Jupyter Notebook</a>]</p>
            <br>
        </div>

        <hr>
        <h2><strong>Evaluation of Model Baselines</strong></h2>
        <h3><strong>Task 1</strong>: Image-Level Classification of Brain Area</h3>
        <table>
            <thead>
                <tr>
                    <th></th>
                    <th>ROI - C1</th>
                    <th>ROI - C2</th>
                    <th>ROI - C3</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Supervised<br>Sup w/ Mixup</td>
                    <td>0.88 ± 0.03<br>0.90 ± 0.04</td>
                    <td>0.77 ± 0.04<br>0.78 ± 0.04</td>
                    <td>0.88 ± 0.02<br>0.90 ± 0.02</td>
                </tr>
                <tr>
                    <td>BYOL<br>MYOW<br>MYOW-m</td>
                    <td>0.88 ± 0.02<br>0.90 ± 0.02<br>0.94 ± 0.02</td>
                    <td>0.75 ± 0.03<br>0.77 ± 0.06<br>0.76 ± 0.04</td>
                    <td>0.97 ± 0.01<br>0.98 ± 0.01<br>0.98 ± 0.01</td>
                </tr>
                <tr>
                    <td>PCA<br>NMF</td>
                    <td>0.59<br>0.62</td>
                    <td>0.24<br>0.25</td>
                    <td>0.07<br>0.50</td>
                </tr>
            </tbody>
        </table>
    
        <p>Download the model weights for Task 1 here: [<a href="https://www.dropbox.com/sh/o5r1e9gvktghdn4/AABzq40VAqTUKoFh_SoBpZkOa?dl=0" target="_blank">Dropbox</a>]</p>

        <br>

        <h3><strong>Task 2</strong>: Pixel-level Segmentation of Microstructures</h3>
        <table>
            <caption><h4>2D Pixel-level Segmentation</h4></caption> 
            <thead>
                <tr>
                    <th></th>
                    <th colspan=4>3-Class</th>
                    <th colspan=5>4-Class without ZI</th>
                    </tr>
                    <tr class="small-font">
                    <th style="width:16%">Method</th>
                    <th>Bg + Axons</th>
                    <th>Vessels</th>
                    <th>Cells</th>
                    <th>Avg.</th>
                    <th>Bg</th>
                    <th>Vessels</th>
                    <th>Cells</th>
                    <th>Axons</th>
                    <th>Avg.</th>
                </tr>
            </thead>
            <tbody>
                <tr class="small-font">
                    <td>2D U-Net (F1)<br>2D U-Net (IoU)</td>
                    <td>0.99<br>0.98</td>
                    <td>0.76<br>0.64</td>
                    <td>0.85<br>0.75</td>
                    <td>0.87 ± 0.012<br>0.79 ± 0.014</td>
                    <td>0.97<br>0.89</td>
                    <td>0.82<br>0.70</td>
                    <td>0.87<br>0.77</td>
                    <td>0.94<br>0.60</td>
                    <td>0.90 ± 0.003<br>0.74 ± 0.008</td>
                </tr>
                <tr class="small-font">
                    <td>MAnet (F1)<br>MAnet (IoU)</td>
                    <td>0.99<br>0.98</td>
                    <td>0.79<br>0.68</td>
                    <td>0.87<br>0.78</td>
                    <td>0.88 ± 0.003<br>0.81 ± 0.003</td>
                    <td>0.97<br>0.89</td>
                    <td>0.83<br>0.71</td>
                    <td>0.87<br>0.78</td>
                    <td>0.94<br>0.76</td>
                    <td>0.90 ± 0.002<br>0.78 ± 0.011</td>
                </tr>
                <tr class="small-font">
                    <td>FPN (F1)<br>FPN (IoU)</td>
                    <td>0.99<br>0.97</td>
                    <td>0.72<br>0.59</td>
                    <td>0.84<br>0.73</td>
                    <td>0.85 ± 0.01<br>0.76 ± 0.015</td>
                    <td>0.96<br>0.87</td>
                    <td>0.73<br>0.59</td>
                    <td>0.84<br>0.72</td>
                    <td>0.93<br>0.72</td>
                    <td>0.86 ± 0.004<br>0.72 ± 0.021</td>
                </tr>
                <tr class="small-font">
                    <td>Unet++ (F1)<br>Unet++ (IoU)</td>
                    <td>0.99<br>0.98</td>
                    <td>0.79<br>0.68</td>
                    <td>0.87<br>0.78</td>
                    <td>0.89 ± 0.002<br>0.81 ± 0.002</td>
                    <td>0.97<br>0.88</td>
                    <td>0.81<br>0.68</td>
                    <td>0.85<br>0.75</td>
                    <td>0.93<br>0.73</td>
                    <td>0.89 ± 0.015<br>0.76 ± 0.036</td>
                </tr>
                <tr class="small-font">
                    <td>PAN (F1)<br>PAN (IoU)</td>
                    <td>0.97<br>0.94</td>
                    <td>0.60<br>0.46</td>
                    <td>0.80<br>0.66</td>
                    <td>0.79 ± 0.044<br>0.69 ± 0.052</td>
                    <td>0.95<br>0.85</td>
                    <td>0.69<br>0.53</td>
                    <td>0.80<br>0.67</td>
                    <td>0.93<br>0.76</td>
                    <td>0.84 ± 0.007<br>0.70 ± 0.014</td>
                </tr>
                <tr class="small-font">
                    <td>PSPNet (F1)<br>PSPNet (IoU)</td>
                    <td>0.97<br>0.94</td>
                    <td>0.48<br>0.39</td>
                    <td>0.74<br>0.61</td>
                    <td>0.73 ± 0.013<br>0.65 ± 0.043</td>
                    <td>0.94<br>0.82</td>
                    <td>0.54<br>0.38</td>
                    <td>0.71<br>0.55</td>
                    <td>0.91<br>0.74</td>
                    <td>0.78 ± 0.012<br>0.62 ± 0.015</td>
                </tr>
            </tbody>
        </table>
        <br>
        <table>
            <caption><h4>3D Pixel-level Segmentation</h4></caption> 
            <thead>
                <tr>
                    <th></th>
                    <th colspan=4>3-Class</th>
                    <th colspan=5>4-Class without ZI</th>
                    </tr>
                    <tr class="small-font">
                    <th style="width:16%">Method</th>
                    <th>Bg + Axons</th>
                    <th>Vessels</th>
                    <th>Cells</th>
                    <th>Avg.</th>
                    <th>Bg</th>
                    <th>Vessels</th>
                    <th>Cells</th>
                    <th>Axons</th>
                    <th>Avg.</th>
                </tr>
            </thead>
            <tbody>
                <tr class="small-font">
                    <td>3D U-Net (F1)<br>3D U-Net (IoU)</td>
                    <td>0.99<br>0.98</td>
                    <td>0.77<br>0.65</td>
                    <td>0.87<br>0.76</td>
                    <td>0.88 ± 0.006<br>0.80 ± 0.007</td>
                    <td>0.93<br>0.81</td>
                    <td>0.76<br>0.62</td>
                    <td>0.80<br>0.67</td>
                    <td>0.87<br>0.50</td>
                    <td>0.84 ± 0.032<br>0.65 ± 0.045</td>
                </tr>
                <tr class="small-font">
                    <td>VNetLight (F1)<br>VNetLight (IoU)</td>
                    <td>0.99<br>0.97</td>
                    <td>0.75<br>0.61</td>
                    <td>0.83<br>0.70</td>
                    <td>0.85 ± 0.012<br>0.76 ± 0.013</td>
                    <td>0.90<br>0.78</td>
                    <td>0.65<br>0.46</td>
                    <td>0.73<br>0.58</td>
                    <td>0.76<br>0.43</td>
                    <td>0.76 ± 0.063<br>0.56 ± 0.061</td>
                </tr>
                <tr class="small-font">
                    <td>HighResNet (F1)<br>HighResNet (IoU)</td>
                    <td>0.99<br>0.97</td>
                    <td>0.74<br>0.61</td>
                    <td>0.84<br>0.72</td>
                    <td>0.85 ± 0.019<br>0.77 ± 0.026</td>
                    <td>0.89<br>0.73</td>
                    <td>0.51<br>0.35</td>
                    <td>0.73<br>0.58</td>
                    <td>0.77<br>0.42</td>
                    <td>0.72 ± 0.083<br>0.52 ± 0.075</td>
            </tbody>
        </table>

        <h4> Task 2 Notebook: Using Pretrained Weights </h4>
        <p>Refer to this quick notebook for the easy download and application of the Task 2 model weights: [<a href="https://github.com/MTNeuro/MTNeuro/blob/temp_branch/notebooks/task2_using_pretrained_models.ipynb" target="_blank">Notebook: Using Pretrained Weights</a>] <br>
        It also demonstrates the use of the additional [<a href="https://github.com/MTNeuro/MTNeuro/tree/temp_branch/notebooks/scripts" target="_blank">utility scripts</a>] included.</p>
        <p>Or alternatively, download the model weights for Task 2 manually here: [<a href="https://www.dropbox.com/sh/pzza5nuh93r9s18/AAAZsISLUl1H_u3U0TDSeOjNa?dl=0" target="_blank">Dropbox</a>]</p>
        
        
        <br>
        <h3><strong>Task 3</strong>: Multi-task Decoding from Frozen Representations</h3>
        <table>
            <caption><h4>Linear Readouts from Models Trained on a Single Subvolume (ROI-C1)</h4></caption>
            <thead>
                <tr>
                    <th>Method</th>
                    <th>Vessels</th>
                    <th>Axons</th>
                    <th>Cell Count</th>
                    <th>Cell Size</th>
                    <th>Dist (k=1)</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Supervised<br>Sup w/ Mixup</td>
                    <td>0.77 ± 0.06<br>0.82 ± 0.02</td>
                    <td>0.94 ± 0.01<br>0.95 ± 0.00</td>
                    <td>0.67 ± 0.06<br>0.71 ± 0.02</td>
                    <td>0.61 ± 0.05<br>0.67 ± 0.03</td>
                    <td>0.48 ± 0.05<br>0.47 ± 0.02</td>
                </tr>
                <tr>
                    <td>BYOL<br>MYOW<br>MYOW-m</td>
                    <td>0.85 ± 0.01<br>0.85 ± 0.01<br>0.87 ± 0.01</td>
                    <td>0.94 ± 0.01<br>0.94 ± 0.01<br>0.95 ± 0.01</td>
                    <td>0.75 ± 0.01<br>0.74 ± 0.01<br>0.77 ± 0.01</td>
                    <td>0.69 ± 0.01<br>0.69 ± 0.01<br>0.69 ± 0.01</td>
                    <td>0.49 ± 0.01<br>0.50 ± 0.02<br>0.51 ± 0.01</td>
                </tr>
                <tr>
                    <td>PCA<br>NMF</td>
                    <td>0.75<br>0.81</td>
                    <td>0.82<br>0.85</td>
                    <td>0.55<br>0.59</td>
                    <td>0.47<br>0.55</td>
                    <td>0.31<br>0.34</td>
                </tr>
            </tbody>
        </table>
        <br>
        <table>
            <caption><h4>Linear Readouts from Models Trained on Two Subvolumes (ROI-C3)</h4></caption>
            <thead>
                <tr>
                    <th>Method</th>
                    <th>Vessels</th>
                    <th>Axons</th>
                    <th>Cell Count</th>
                    <th>Cell Size</th>
                    <th>Dist (k=1)</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Supervised<br>Sup w/ Mixup</td>
                    <td>0.79 ± 0.02<br>0.75 ± 0.04</td>
                    <td>0.94 ± 0.02<br>0.88 ± 0.04</td>
                    <td>0.73 ± 0.02<br>0.64 ± 0.04</td>
                    <td>0.63 ± 0.04<br>0.54 ± 0.07</td>
                    <td>0.49 ± 0.02<br>0.37 ± 0.05</td>
                </tr>
                <tr>
                    <td>BYOL<br>MYOW<br>MYOW-m</td>
                    <td>0.88 ± 0.00<br>0.88 ± 0.01<br>0.87 ± 0.01</td>
                    <td>0.96 ± 0.00<br>0.96 ± 0.00<br>0.96 ± 0.01</td>
                    <td>0.79 ± 0.00<br>0.79 ± 0.01<br>0.78 ± 0.01</td>
                    <td>0.73 ± 0.01<br>0.72 ± 0.01<br>0.72 ± 0.01</td>
                    <td>0.53 ± 0.02<br>0.52 ± 0.01<br>0.53 ± 0.01</td>
                </tr>
                <tr>
                    <td>PCA<br>NMF</td>
                    <td>0.75<br>0.75</td>
                    <td>0.82<br>0.83</td>
                    <td>0.53<br>0.56</td>
                    <td>0.46<br>0.49</td>
                    <td>0.29<br>0.31</td>
                </tr>
            </tbody>
        </table>

        <p>The model weights for Task 3 are the same as for Task 1. Download them here: [<a href="https://www.dropbox.com/sh/o5r1e9gvktghdn4/AABzq40VAqTUKoFh_SoBpZkOa?dl=0" target="_blank">Dropbox</a>]</p>

        <br>
        <hr>

  
        <h2><strong>Citation</strong></h2>
        <p>If you find this dataset or benchmark useful in your research, please cite the following papers:</p>
        <code>
            Quesada, J., Sathidevi, L., Liu, R., Ahad, N., Jackson, J.M., Azabou, M., ... & Dyer, E. L. (2022). 
            MTNeuro: A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of Abstraction. 
            Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track.
        </code>
        <br>
        <br>
        <code>
            Prasad, J. A., Balwani, A. H., Johnson, E. C., Miano, J. D., Sampathkumar, V., De Andrade, V., ... & Dyer, E. L. (2020). 
            A three-dimensional thalamocortical dataset for characterizing brain heterogeneity. Scientific Data, 7(1), 1-7.
        </code>
        <br>
        <br>
        <hr>

        <h2><strong>License</strong></h2>
        <p>This software is available under the <a href="https://opensource.org/licenses/MIT" target="_blank">MIT License</a>.<br>The X-ray Microtomography image dataset is licensed under Creative Commons Attribution 4.0 International (CC BY 4.0).</p>
        <br>
        <hr>


    <br>

    <div id="footer">
        <div class="lab-flexbox">
            <div class="item">
                <h3 style="text-align:center"><a href="https://dyerlab.gatech.edu/" target="_blank">Neural Data Science Lab</a><br>Georgia Institute of Technology</h3>
                <p><strong>Contributors:</strong><br>Jorge Quesada, Lakshmi Sathidevi, Ran Liu, Nauman Ahad, Joy M. Jackson, Mehdi Azabou, Jingyun Xiao, Christopher Liding, Carolina Urzay, Eva L. Dyer</p>
            </div>
            <div class="item">
                <img src="images/nerds_logo.png" alt="NerDS Lab Logo" class="lab-logo">
            </div>
            <div class="item">
                <img src="images/boss_logo.png" alt="BossDB Logo" class="lab-logo">
            </div>
            <div class="item">
                <h3 style="text-align:center"><a href="https://bossdb.org/" target="_blank">Brain Observatory Storage Service & Database</a><br>Johns Hopkins University Applied Physics Laboratory</h3>
                <p><strong>Contributors:</strong><br>William Gray-Roncal<br>Erik C. Johnson</p>
            </div>
            
        </div>

        <p>For inquiries, contact Eva L. Dyer (evadyer@gatech.edu), Erik C. Johnson (erik.c.johnson@jhuapl.edu), or Jorge Quesada (jpacora3@gatech.edu).</p>
        <p>Website hosted on <a href="https://github.com/MTNeuro/mtneuro.github.io" target="_blank">Github</a>. Updated August 2022.</p>
    </div>

    <script src="task_descriptions.js"></script>

    </body>


</html>
